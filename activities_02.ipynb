{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os\n", "import json\n", "import nltk\n", "nltk.download('stopwords')\n", "from nltk.tokenize import word_tokenize\n", "from nltk.corpus import stopwords\n", "from nltk.stem import PorterStemmer\n", "from pathlib import Path\n", "from collections import defaultdict\n", "import torch\n", "import numpy as np\n", "import spacy\n", "import matplotlib.pyplot as plt\n", "import regex as re"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def tokenize_json(file_path):\n", "    with open (file_path, 'r', encoding=\"utf-8\") as f:\n", "        data = json.load(f)\n", "        text = data['text']\n", "        text = text.lower()\n", "        tokens = nltk.word_tokenize(text)\n", "        stop_words = set(stopwords.words('portuguese'))\n", "        tokens = [token for token in tokens if token not in stop_words]\n", "        print(tokens)\n", "    return tokens\n", "file = 'dois/342.json'"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def tokenize_multiples_json(directory):\n", "    all_tokens = []\n", "    vocab = defaultdict(lambda: len(vocab))\n", "    for file in os.listdir(directory):\n", "        if file.endswith('.json'):\n", "            file_path = os.path.join(directory, file)\n", "            tokens = tokenize_json(file_path)\n", "            tokens_ids = [vocab[token] for token in tokens]\n", "            all_tokens.append(tokens_ids)\n", "            #print(tokens_ids)\n", "            #print('tama\u00c3\u00b1o:', len(tokens_ids))\n", "    return all_tokens"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["directory = \"dois\"\n", "tokenize_multiples_json(directory)\n", "all_tokens = tokenize_multiples_json(directory)   \n", "print('length of all tokens:',len(all_tokens))     "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def get_stats(tokens):\n", "    counts = {}\n", "    for i in range(len(tokens)- 1):\n", "        pair = (tokens[i], tokens[i+1])\n", "        counts[pair] =  counts.get(pair, 0) + 1\n", "        return counts\n", "stats = get_stats(all_tokens[0])\n", "print (stats)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["top_pair = max(stats, key=stats.get)\n", "print('Top pair:', top_pair)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def merge(ids, pair, idx):\n", "    newids = []\n", "    i = 0\n", "    while i< len(ids):\n", "        if i <len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n", "            newids.append(idx)\n", "            i+=2\n", "        else:\n", "            newids.append(ids[i])\n", "            i+=1\n", "    return newids\n", "    \n", "tokens2 = merge(all_tokens, top_pair, 256) \n", "print('Tokens2:', tokens2)\n", "print('length of tokens2:', len(tokens2))   "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["vocab_size = 276\n", "num_merges = vocab_size - 256\n", "ids = list(all_tokens[0])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["merges = {}\n", "for i in range(num_merges):\n", "    stats = get_stats(ids)\n", "    pair = max(stats, key=stats.get)\n", "    idx = 256 + len(merges)\n", "print(f\"merging {pair} into a new token {idx}\")\n", "ids = merge(ids, pair, idx)\n", "merges [pair] = idx    "]}, {"cell_type": "markdown", "metadata": {}, "source": ["rint(\"tokens length:\", len(tokens))<br>\n", "rint(\"ids length:\", len(ids))<br>\n", "rint(f\"compression ratio: {len(tokens) / len(ids):2f}X\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["ecoding"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["vocab = {idx: bytes ([idx] )for idx in range(256) }\n", "for (p0, p1), idx in merges.items():\n", "    vocab[idx] = vocab [p0] + vocab [p1]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def decode (ids): \n", "    tokens = b\"\".join(vocab[idx] for idx in ids)\n", "    text = tokens.decode(\"utf-8\", errors=\"replace\")\n", "    return text\n", "print(decode([73]))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["ncoding"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def encode(text):\n", "    tokens =  list(text.encode(\"utf-8\"))\n", "    while True:\n", "        stats =  get_stats(tokens)\n", "        pair = min(stats, key= lambda p: merges.get(p, float (\"inf\")))\n", "        if pair not in merges:\n", "            break\n", "        idx = merges[pair]\n", "        tokens =  merge(tokens, pair, idx)\n", "    return tokens\n", "print(encode(\"O tempo \u00c3\u00a9 muito poderoso!\"))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["onta com que frequencia ocurrem as combina\u00c3\u00a7\u00c3\u00b5es"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["b = {}\n", "for words in all_tokens[:1]:\n", "    ch = ['<S>'] + list(words) + ['<E>']\n", "    for ch1, ch2 in zip(words, words[1:]):\n", "        bigram = (ch1, ch2)\n", "        b[bigram] = b.get(bigram, 0) +1\n", "        #print(ch1, ch2)\n", "        \n", "a = sorted(b.items(), key= lambda kv: -kv[1])\n", "#print('print sorted:', a)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["sequences = all_tokens\n", "max_len = max(len(seq) for seq in sequences)\n", "padded_sequences = [seq + [0] *(max_len - len(seq))for seq in sequences]\n", "matriz = torch.tensor((padded_sequences), dtype= torch.int32)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["forma = matriz.shape\n", "print ('a matriz torcs:', forma)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["submatriz = matriz [:10, :10]\n", "matriz_float = np.array(submatriz, dtype=np.float32)\n", "np.set_printoptions(suppress=True)\n", "print('Submatriz:', matriz_float)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["p = matriz_float\n", "p = p/p.sum()\n", "print ('Calculo:', p)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def def_perplexity (vector_probability):\n", "    vector_probability = np.where(vector_probability ==0, 1e-10, vector_probability)\n", "    entropia = -np.sum(vector_probability * np.log2(vector_probability))\n", "    perplexity = np.exp2(entropia)\n", "    return perplexity"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["vector_prob = np.array(p)\n", "perplexity = def_perplexity(vector_prob)\n", "print('La perplexidad es:', perplexity)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}