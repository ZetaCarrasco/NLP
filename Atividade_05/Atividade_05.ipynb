{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bff8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37a7bac",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "#Carrega o tokenizer e o modelo GPT-2 pre-treinado\n",
    "meu_modelo = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(meu_modelo)\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "model = GPT2LMHeadModel.from_pretrained(meu_modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1136b498",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class MyCustomModel(nn.Module):\n",
    "    def __init__(self, gpt2_model):\n",
    "        super(MyCustomModel, self).__init__()\n",
    "        self.gpt2 = gpt2_model\n",
    "        #Adicione outras camadas ou modificações aqui\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        #Use o GPT-2 como parte do sei=u modelo\n",
    "        outputs = self.gpt2(input_ids = input_ids, attention_mask = attention_mask)\n",
    "        return outputs    \n",
    "    def generate(self, input_ids, attention_mask=None):\n",
    "        return self.gpt2.generate(input_ids, attention_mask)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77391c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = MyCustomModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e48ad38",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "optimizer = torch.optim.AdamW(my_model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee157a6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Carrega um conjunto de dados de exemplo (usando o dataset \"wikitext\" da Hugging Face)\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effdc549",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Função para tokenizar o conjunto de dados\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76470f66",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Aplica a tokenização ao conjunto de dados\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0d69be",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Converte o conjunto de dados tokenizado em um DataLoader\n",
    "# Aqui, precisamos empacotar os tensores manualmente\n",
    "def collate_fn(batch):\n",
    "    input_ids = [torch.tensor(item[\"input_ids\"], dtype=torch.long) for item in batch]  \n",
    "    attention_mask = [torch.tensor(item[\"attention_mask\"], dtype=torch.long) for item in batch]  \n",
    "    return {\n",
    "        \"input_ids\": torch.stack(input_ids),  \n",
    "        \"attention_mask\": torch.stack(attention_mask),  \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17713a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converte o conjunto de dados tokenizado em um DataLoader\n",
    "dataloader = DataLoader(tokenized_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab3c638",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "my_model = MyCustomModel(model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34eec0a1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Função para gerar a próxima palavra\n",
    "def gerar_proxima_palavra(model, tokenizer, prompt, max_length=50):\n",
    "    # Tokeniza o prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Gera a próxima palavra (ou sequência de palavras)\n",
    "    output = model.gpt2.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=max_length, \n",
    "        num_return_sequences=1, \n",
    "        no_repeat_ngram_size=2,  \n",
    "        do_sample=True,  \n",
    "        top_k=50, \n",
    "        top_p=0.95,  \n",
    "    )\n",
    "\n",
    "    # Decodifica a saída para texto\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586087f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de uso\n",
    "prompt = \"No Brasil as pessoas gostam de\"  \n",
    "generated_text = gerar_proxima_palavra(my_model, tokenizer, prompt)\n",
    "print(\"Texto gerado:\", generated_text)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
