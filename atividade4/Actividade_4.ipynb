{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10022434",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23695921",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Função para carregar o dataset\n",
    "def read_data_set(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        return df \n",
    "    except Exception as e: \n",
    "        print(f\"Erro ao carregar o dataset: {e}\")\n",
    "    print(f\"Dataset carregado com sucesso: {file_path}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7b8789",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Função para dividir o dataset\n",
    "def split_data(df):\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)\n",
    "    print(f\"Treino: {len(train_df)}, Validação: {len(val_df)}, Teste: {len(test_df)}\")\n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc8b136",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Classe CustomDataset\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(text, return_tensors='pt', max_length=self.max_length, padding='max_length', truncation=True)\n",
    "        input_ids = encoding['input_ids'].squeeze(0)\n",
    "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0849a3b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Função collate_fn\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    labels = torch.stack([item['labels'] for item in batch])\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372af31c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Função para calcular a acurácia\n",
    "def compute_accuracy(preds, labels):\n",
    "    preds = torch.argmax(preds, dim=1)\n",
    "    return (preds == labels).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b48762e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Função de avaliação\n",
    "def evaluate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_accuracy += compute_accuracy(logits, labels).item()\n",
    "\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            labels = labels.cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "    avg_val_loss = total_loss / len(val_loader)\n",
    "    avg_val_accuracy = total_accuracy / len(val_loader)\n",
    "    f1_micro = f1_score(all_labels, all_preds, average='micro')\n",
    "    f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
    "    \n",
    "    # Matriz de confusão\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "    print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "    \n",
    "    return avg_val_loss, avg_val_accuracy, f1_micro, f1_macro, all_labels, all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a12180",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Função de treinamento\n",
    "def train(model, train_loader, val_loader, optimizer, criterion, device, epochs=3):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        total_loss = 0\n",
    "        total_accuracy = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_accuracy += compute_accuracy(logits, labels).item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        avg_train_accuracy = total_accuracy / len(train_loader)\n",
    "        print(f\"Train Loss: {avg_train_loss:.4f}, Train Accuracy: {avg_train_accuracy:.4f}\")\n",
    "\n",
    "        avg_val_loss, avg_val_accuracy, f1_micro, f1_macro, all_labels, all_preds = evaluate(model, val_loader, criterion, device)\n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {avg_val_accuracy:.4f}\")\n",
    "        print(f\"F1 Micro: {f1_micro:.4f}, F1 Macro: {f1_macro:.4f}\")\n",
    "        \n",
    "        return f1_micro, f1_macro, all_labels, all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6b1581",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Função principal\n",
    "def process_datasets(file_path):\n",
    "    df = read_data_set(file_path)\n",
    "    if df is None:\n",
    "        return None\n",
    "    \n",
    "    train_df, val_df, test_df = split_data(df)\n",
    "\n",
    "    # Mapeamento de classes\n",
    "    class_mapping = {label: idx for idx, label in enumerate(train_df['class'].unique())}\n",
    "    print(f\"Class Mapping: {class_mapping}\")\n",
    "\n",
    "    # Aplicar mapeamento\n",
    "    train_df['class'] = train_df['class'].map(class_mapping)\n",
    "    val_df['class'] = val_df['class'].map(class_mapping)\n",
    "    test_df['class'] = test_df['class'].map(class_mapping)\n",
    "\n",
    "    # Tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Datasets\n",
    "    train_dataset = CustomDataset(train_df['text'].tolist(), train_df['class'].tolist(), tokenizer)\n",
    "    val_dataset = CustomDataset(val_df['text'].tolist(), val_df['class'].tolist(), tokenizer)\n",
    "    test_dataset = CustomDataset(test_df['text'].tolist(), test_df['class'].tolist(), tokenizer)\n",
    "\n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # Modelo\n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(class_mapping))\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Otimizador e função de perda\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Treinamento\n",
    "    return train(model, train_loader, val_loader, optimizer, criterion, device, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e21ae8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processar os datasets\n",
    "file_path1 = 'Dmoz-Sports.csv'\n",
    "file_path2 = 'SyskillWebert.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ff2cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nProcessando Dmoz-Sports:\")\n",
    "f1_micro_1, f1_macro_1, labels_1, preds_1 = process_datasets(file_path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a80479",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nProcessando SyskillWebert:\")\n",
    "f1_micro_2, f1_macro_2, labels_2, preds_2 = process_datasets(file_path2)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
